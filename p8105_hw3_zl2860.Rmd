---
title: "p8105_hw3_zl2860"
author: "Zongchao Liu"
date: "10/6/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(patchwork)
library(ggridges)
library(p8105.datasets)

# settings for markdown
knitr::opts_chunk$set(echo = TRUE,
  warning = FALSE,
	fig.width = 10, 
  fig.height = 8,
  out.width = "90%")
```

# Problem 1
## load data
Let's see a brief summary of this dataset
```{r, echo=FALSE}
#load data for problem 1 
data("instacart")
skimr::skim(instacart)
```

The `instacart` dataset has `r nrow(instacart)` observations of `r ncol(instacart)` variables. The brief summary of the data shows that the datatypes inclue both the character and the integer. However, some of the variables are not in the correct type for feature analysis(i.e. `*_id`), requiring cleaning based on our need. Key variables include `aisle`,`deaprment`,`product_name`,`order_dow`, `order_hour_of_day` and others.

Illustrative examples of observation:
e.g.
The first row indicates that the customer with id 112108 bought a Bulgarian Yogurt at `yogurt` aisle from the `diary eggs` department. The id of the aisle is 120 and the id of the deparmtent is 16. This product was ordered during the 10th hour on the 4 th day of the week. Days since the last order are 9 and the order sequence number for this user is 4 th. This prodcut has been ordered by this user in the past. The evaluation set this order belongs in is `train`.

## Answer the questions:

### 1. How many aisles are there, and which aisles are the most items ordered from?

```{r}
instacart %>%
  summarise(number_of_aisles = n_distinct(aisle)) %>%
  pull(number_of_aisles)# 134 aisles

instacart %>%
  group_by(aisle)%>%
  summarise(items_number = n()) %>%
  filter(items_number == max(items_number)) %>%
  pull(aisle)
```

Answer: There are 134 aisles and the most items are ordered from the `fresh vegetables` aisle.



### 2. Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

```{r,echo=FALSE}
instacart %>%
  group_by(aisle) %>%
  summarise(items_number = n()) %>%
  filter(items_number > 10000) %>%
  ggplot(aes(x = reorder(aisle, items_number), y = items_number, fill = aisle)) +
  geom_col() +
  viridis::scale_fill_viridis(option = "viridis" , discrete = TRUE) +
  labs(
    title = "Number of Items",
    x = "Aisles",
    y = "number of items"
  ) +
  theme_light()+
  theme(plot.title = element_text(hjust = 0.5)) +
  coord_flip()+
  theme(legend.position = "none")
```

### 3. Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

```{r,echo=FALSE}
three_aisles <- 
  instacart %>%
  filter(aisle == "baking ingredients" |
         aisle == "dog food care" |
         aisle == "packaged vegetables fruits") %>%
  group_by(aisle,product_name) %>%
  summarise(items_number = n())

baking_ingredients<- 
  three_aisles %>%
  filter(aisle == "baking ingredients") %>%
  arrange(desc(items_number)) %>%
  head(3) 

dog_food_care<- 
  three_aisles %>%
  filter(aisle == "dog food care") %>%
  arrange(desc(items_number)) %>%
  head(3)

packaged_vegetables_fruits<- 
  three_aisles %>%
  filter(aisle == "packaged vegetables fruits") %>%
  arrange(desc(items_number)) %>%
  head(3)

  bind_rows(baking_ingredients, dog_food_care) %>%
  bind_rows(packaged_vegetables_fruits) %>%
  knitr::kable(
    caption = "Table 1"
  )
```

### 4. Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table)

```{r,echo=FALSE}
days <- c("Sunday", "Monday","Tuesday","Wednesday","Thursday","Friday","Saturday")

instacart %>%
  filter( product_name == "Pink Lady Apples" | 
          product_name == "Coffee Ice Cream") %>%
  select(order_dow, order_hour_of_day, product_name) %>%
  mutate(order_dow =recode(order_dow, 
         "0" = "Sunday", 
         "1" = "Monday",
         "2" = "Tuesday",
         "3" = "Wednesday",
         "4" = "Thursday",
         "5" = "Friday",
         "6" = "Saturday"),
         order_dow = factor(order_dow, levels = days)) %>%
  group_by(order_dow, product_name) %>%
  summarize(mean_hour = round(mean(order_hour_of_day), digits = 1)) %>%
  pivot_wider(names_from = order_dow,
              values_from = mean_hour) %>%
  knitr::kable()
```

# Problem 2

```{r,include=FALSE}
#load data
library(p8105.datasets)
```


```{r,echo=FALSE}
BRFSS <- brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  filter(response == "Excellent" |
         response ==  "Good" |
         response == "Very good"|
         response == "Fair" |
         response == "Poor"
         ) %>%
  mutate(response = factor(response, levels = c("Poor",
                                       "Fair",
                                       "Good",
                                       "Very good",
                                       "Excellent")))
```

## answer the questions:

### 1. In 2002, which states were observed at 7 or more locations? What about in 2010?
```{r,echo=FALSE}
states2002<-
  BRFSS %>%
  filter(year == 2002) %>%
  group_by(locationabbr) %>%
  distinct(locationdesc) %>%
  summarize(obs_loc = n()) %>%
  filter(obs_loc >= 7) 

states2002 %>%
  knitr::kable()

states2010 <-
  BRFSS %>%
  filter(year == 2010) %>%
  group_by(locationabbr) %>%
  distinct(locationdesc) %>%
  summarize(obs_loc = n()) %>%
  filter(obs_loc >= 7) 

states2010%>%
  knitr::kable()
  
```
As the tables show:

In 2002, `r pull(states2002,locationabbr)` were observed at 7 or more locations.

In 2010,  `r pull(states2010,locationabbr)` were observed at 7 or more locations.


### 2. Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).

```{r,echo=FALSE}
BRFSS %>%
  filter(response == "Excellent") %>% #limited to Excellent responses
  group_by(locationabbr, year) %>%  #contains year, state
  mutate(avg_data_value = mean(data_value)) %>% 
  select(year, locationabbr, avg_data_value) %>% #create a variable that averages the data_value across locations within a state
  distinct() %>%
  ggplot(aes(x = year , y = avg_data_value, color = locationabbr, group = locationabbr)) +
  geom_line() + 
  viridis::scale_color_viridis(discrete = TRUE) +
  theme_bw()
  

```

From the plot above we can see that for most of the states, the average `data_value`s are within the range of 17.5 ~ 27.5 and flutuate across the years. 


### 3. Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

```{r,echo=FALSE}
# distribution with counties
#plot for 2006
plot_2006 <-
  BRFSS %>%
  filter(year == 2006,
         locationabbr =="NY") %>%
  ggplot(aes(x = locationdesc,
             y = data_value,
             group = response,
             color = response)) +
  geom_point(aes(size = data_value),
                 alpha =.7) +
  geom_line() +
  theme_bw() +
  labs(
    title = "Distribution of `data_value` for Responses",
    subtitle = "2006 in NY State",
    x = "Counties",
    y = "Data value"
  ) +
  theme(legend.position = "bottom") +
  theme(plot.title = element_text(hjust = 0.5), 
        plot.subtitle = element_text(hjust = 0.5),
        legend.position = "none", 
        axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.5))

#plot for 2010
plot_2010 <-
  BRFSS %>%
  filter(year == 2010,
         locationabbr =="NY") %>%
  ggplot(aes(x = locationdesc,
             y = data_value,
             group = response,
             color = response)) +
  geom_point(aes(size = data_value),
                 alpha =.7) +
  geom_line() +
  theme_bw() +
  labs(
    title = "Distribution of `data_value` for Responses",
    subtitle = "2010 in NY State",
    x = "Counties",
    y = "Data value"
  ) +
  theme(legend.position = "none") +
  theme(plot.title = element_text(hjust = 0.5), 
        plot.subtitle = element_text(hjust = 0.5),
        legend.position = "bottom", 
        axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.5))

plot_2006 + plot_2010
```

I also made another plot here to show the density of the `data_value`. However, in this plot, the names of the counties are not specified. The two plots respectively reflect different information.

```{r, echo = FALSE}
# distribution without counties
plot_2006_density <- 
  BRFSS %>%
  filter(year == 2006,
         locationabbr == "NY") %>%
  select(year, response, data_value)  %>%
  ggplot(aes(x =data_value,
             y = response,
             fill = response)) + 
  geom_density_ridges2 (scale = .9,
                       alpha =.7) +
  theme_bw()+
  labs(
    title = str_to_title("the distribution of data_value for responses"),
    subtitle = "NY, 2006",
    x = "Data value",
    y ="Density"
  ) +
  theme(legend.position = "bottom"  ) +
  viridis::scale_fill_viridis(discrete = TRUE)

plot_2010_density <-
  BRFSS %>%
  filter(year == 2010,
         locationabbr == "NY") %>%
  select(year, response, data_value)  %>%
  ggplot(aes(x =data_value,
             y = response,
             fill = response)) + 
  geom_density_ridges2 (scale = 0.9,
                       alpha =.7) +
  theme_bw()+
  labs(
    title = str_to_title("the distribution of data_value for responses"),
    subtitle = "NY, 2010",
    x = "Data value",
    y ="Density"
  )+
  theme(legend.position = "none"  ) +
  viridis::scale_fill_viridis(discrete = TRUE)

(plot_2006_density + plot_2010_density)
```



# problem 3
## Q1. Load, tidy, and otherwise wrangle the data.
```{r,echo=FALSE}
#load and clean data
day_ind <-
  seq(1,29,7) # create a vector for easily adjusting `day_id` later

accel_data <- 
  read_csv("./dataset/accel_data.csv") %>%
  janitor::clean_names() %>%
  mutate(weekday_vs_weekend = ifelse(day == "Saturday" | day == "Sunday", "weekend", "weekday"), #include a weekday vs weekend variable
         day = factor(day, levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
         )  %>%  # change the data type of `day` so that we can rearrange the `day_id`
  arrange(day) %>%
  mutate(
  day_id = c(day_ind, 
             day_ind + 1, 
             day_ind + 2,
             day_ind + 3,
             day_ind + 4,
             day_ind + 5,
             day_ind + 6)  # match `day_id` with the order of the 35 days in a human friendly way
  ) %>%
  arrange(week) #check if the `day_id` is correctly cleaned
```

The dataset `accel_data` has `r nrow(accel_data)` observations of `r ncol(accel_data) `, containing the activity information in each minute of 35 days. Also, index of weeks and days are shown by the dataset. Here, I rearrange the `day_id` so that the values within this variable are matched with the day of the week, which is an important preparation for future analysis.


## Q2. Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

```{r,echo=FALSE}
# calculate total activity of a day
act_trend <-
  accel_data %>%
  mutate( day = day,
          week = as.character(week)) %>%
  pivot_longer( activity_1:activity_1440,
                names_to = "min",
                values_to = "min_activity",
               ) %>%
  group_by(week, day) %>%
  summarise(total_activity = sum(min_activity)) #aggregate accross minutes to create a total activity variable for each day

act_trend %>%
  knitr::kable()

act_trend %>%
  ggplot(aes(x = week, y = total_activity, fill = day)) + 
  geom_col() +
  theme_bw() +
  labs(
    title = "Trends of the Total Activity in 5 Weeks",
    x = "Week",
    y = "Total Activity"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5)
  )
```

Since there are 35 days(the table has too many rows), the table is hard to directly show the trends. Therefore, I add a plot here. From the plot above, which is derived from the table, it is easy to find that the activity of the man on Sundays is keeping decreasing. Also, There is a peak of the activity on the Monday of the third week and a peak of the weekly activity in the third week. Obviously, the activity on Saturdays of the fourth and the fith week is too low to see on the plot. I have checked the data of these two days on the table, and find that the values are actually very low. There should be some reasons to explain this situation.

## Q3. Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.
```{r,echo=FALSE}
# final_plot_1
accel_data %>%
  mutate(day_id = factor(day_id)) %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "min",
    names_prefix = "activity_",
    values_to = "min_activity"
  ) %>%
  mutate(min = factor(min, levels = c(1:1440))) %>%
  ggplot(aes(x = min, 
             y =min_activity, 
             color = day_id)) +
  geom_area(size = 1, alpha = .5) +
  scale_x_discrete(breaks = seq(60, 1440, (1440 / 4)),
                   labels = c("1", "7", "13", "19")) +
  theme(axis.text.x = element_text(angle = 0))  +
  theme_bw() +
  labs(
    title = "Continuous 35-Day Activity",
    subtitle  = "in 1440 minutes, 24 hours",
    x = "Hours",
    y = "Activity"
  ) +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))
  
```

I made two plots here trying to show the trends. 

Firstly I made an area plot to show the trends of the activity of the 35 days in every minute of a day. From the first plot, we can see that differences of activity do exist between different days. Some of the days have higher activity levels than the others. The most important information we can get from this plot is that the activity of nearly all of the 35 days follow a visible trend across the 24 hours. On each day, the activity keeps increasing before the 7th hour, and then fluctuates from the 7th hour to the 21 st hour. After that, the activity decreases again. If the order of the hours is matched with real time, the result will be more believable because it does relect a normal daily life.

Secondly, I madd a time-series heatmap to show the trend of the 24-hour activity. This plot is different from the first plot because it just shows the activity of every hour rather than every minute of a day. We can get the similar conclusion: the activity keeps increasing before the 7th hour and then fluctuates from the 7th hour to the 21 st hour. After that, it dramatically decreases. However, We can also know something new in this plot. The distributions of the activity peaks in each day are different. Some days have two peaks, some have only one, and others have none. Obviously, from the plot we see some days have very low activity. I have checked the activity of those days in the dataset and find that for most of the minutes within those days, the activity per minute is only "1". There are many reasons to explain that. Perhaps it's because the device failed to record data at that time and only returned the activity value as "1".

```{r,echo=FALSE}
# time series heatmap
accel_data %>%
  mutate(day_id = factor(day_id)) %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "min",
    names_prefix = "activity_",
    values_to = "min_activity"
  ) %>%
  group_by(day_id) %>%
  mutate(min = as.numeric(min),
         hour = ceiling(min / 60)
         ) %>%
  group_by(day_id, hour, week, day) %>%
  summarise(hour_act = sum(min_activity)
            ) %>%
  mutate(day_of_week = day) %>% ## reaarange
  ggplot(aes(x = as.factor(hour),
             y = day_id)
         ) +
  geom_tile(aes(fill = hour_act,
                width = 0.9,
                height =0.9,
                ), 
            size = 2.5) +
  viridis::scale_fill_viridis() +
  scale_fill_gradientn(colors = c("white","orange","red")) +  
  scale_x_discrete(breaks = c(1:24),
                   labels = as.character(c(1:24)
                                         )
                   ) +
  labs(title = "Time Seris Heatmap of the Continuous 35-Day-24-Hour Activity",
       x = "Hours",
       y = "Days") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme_light()
```









